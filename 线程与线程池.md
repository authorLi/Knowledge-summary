## ExecutorService相关
### 关于其参数
关于线程池的参数应该都已经知道了
- corePoolSize核心线程数
- maximumPoolSize最大线程数
- keepAliveTime非核心线程存活时间
- timeUnit keepAliveTime单位
- queue队列
- threadFactory线程工厂
- rejectionStrategy拒绝策略

关于这几个参数有一点需要注意**corePoolSize为核心线程数没错，但是maximumPoolSize并不是非核心线程数，而是核心线程数和非核心线程数的综合**，也即非核心线程数的大小为`maximumPoolSize - corePoolSize`

并且每个线程池的执行快慢是和上面这几个参数息息相关的，所以如果问到哪个线程执行的快，不妨先从这些线程池的参数入手

### 不推荐使用已有的线程池
#### 1. newCachedThreadPool
关于newCachedThreadPool，源码是这样写的
```java
public static ExecutorService newCachedThreadPool() {
    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                  60L, TimeUnit.SECONDS,
                                  new SynchronousQueue<Runnable>());
}

public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) {
    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                  60L, TimeUnit.SECONDS,
                                  new SynchronousQueue<Runnable>(),
                                  threadFactory);
}
```
可以看到其核心线程数为0，最大线程数为Integer.MAX_VALUE(2的31次方减一)。这个就已经暴露了这个线程池如果有可能的话将会无限制的创建线程，最终将有可能创建2的31次方减一个线程，这么多的线程被创建将会引起`**CPU占用率过高(可能达到100%)**`。

如果非要问为什么是CPU而不是内存，那么可以告诉你，你还记得线程是什么吗？**线程是CPU最小的调度单位**，现在知道为什么和CPU有关了吧！

所以说这也提示着我们引起CPU过高的原因之一：**可能就是创建的线程数太多了！**
#### 2. newFixedThreadPool
关于newFixedThreadPool，源码如下：
```java
public static ExecutorService newFixedThreadPool(int nThreads) {
    return new ThreadPoolExecutor(nThreads, nThreads,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue<Runnable>());
}

public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) {
    return new ThreadPoolExecutor(nThreads, nThreads,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue<Runnable>(),
                                  threadFactory);
}
```
可以看到这种线程池它的核心线程数等于最大线程数，这将意味着他不存在非核心线程数，那么他其实也不需要去回收无用的线程。其队列为链式的阻塞队列，其最大值为依然为Integer.MAX_VALUE

那么它会引发什么问题呢？  
它会引发OOM问题，也就是说它会内存泄露。为什么呢？因为这个线程池使用的是无参的链式阻塞队列。这个队列的一大特点就是它的长度最长可以达到Integer.MAX_VALUE，也就是说如果任务的数量达到了核心线程数也就是本线程池的最大线程数，那么他们将都会被存储到队列中，且这个队列是非常非常长的，如果此时每个任务的执行时间恰巧又特别的长，长时间的执行不完，然后队列又被塞满后，大量的内存无法被回收，此时就发生了OOM(内存泄漏)。  
其实这里说的内存泄漏不仅仅因为队列满了而引发的，其实根本原因是因为这个线程池使用的队列的大小太大了，如果任务被阻塞或者任务执行时间过长，那么队列将很快被打满，所以会有很多内存被占用而得不到释放才因此引发了OOM(内存泄漏)

#### 3. newSingleThreadExecutor
再来看看newSingleThreadExecutor吧，源码如下：
```java
public static ExecutorService newSingleThreadExecutor() {
    return new FinalizableDelegatedExecutorService
        (new ThreadPoolExecutor(1, 1,
                                0L, TimeUnit.MILLISECONDS,
                                new LinkedBlockingQueue<Runnable>()));
}

public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) {
    return new FinalizableDelegatedExecutorService
        (new ThreadPoolExecutor(1, 1,
                                0L, TimeUnit.MILLISECONDS,
                                new LinkedBlockingQueue<Runnable>(),
                                threadFactory));
}
```
看到了源码就知道了，这其实是一个核心线程数=最大线程数=1的一种特殊的newFixedThreadPool。所以他可能引发的问题就可想而知了，没错，他同样会引发OOM。
### 推荐的线程池
上面说了那么多，不推荐使用jdk里的定义好的线程池，那么我该是用什么呢？  
其实上面几个定义好的线程池有一个共同的特征：就是内部都调用了ThreadPoolExecutor这个类来实现线程池。那么这个问题的答案就出来了，那就是自己实现ThreadPoolExecutor来根据不同的业务场景定义不同的线程池。
### 提交优先级和执行优先级
它的执行优先级为：核心线程->队列->非核心线程  
而执行优先级为：核心线程->非核心线程->队列

所以由上面的顺序可以知道(其实源码也是这么写的)，他在执行的时候是最后才会执行队列中的任务，所以如果因为任务过多引发了拒绝策略，那么它的报错可能不会是在最后面，而可能是在中间(因为是非核心线程不够用才引发了拒绝，然后非核心线程的任务是在队列任务之前执行的)

参考这篇文章即可：https://blog.csdn.net/sea2333/article/details/113092056


## JDK1.6之前的synchronized
当然synchronized是重量级锁，其如果修饰在非静态方法上则相当于用**当前对象**作为一把锁把整个方法的内容包起来。  
而如果修饰在静态方法中则是相当于用**当前类对象(Class对象)**作为一把锁把整个方法内容包起来

下面是JDK1.6之前的synchronized简略流程图
![JDK1.6之前的synchronized大概流程图](https://note.youdao.com/yws/public/resource/d6cc34b170f27b87b2be9824af2cea93/xmlnote/C4C7C23FB42A41E0ADBEA7B1C74A494A/19940)
可以看到JDK1.6之前synchronized是使用了一个队列来装待执行的线程的

并且为什么synchronized是重量级锁也应该一目了然了，没错因为它：
- 可能发生阻塞
- 需要频繁切换上下文
- 还需要操作系统的不断调度

### 那JDK1.6以后的synchronized都做了哪些优化？
由于synchronized太笨重并且开发者发现大多数情况下只有某一个固定的线程去执行"锁住的"代码，所以他们发明了`偏向锁`。  
这个偏向锁在锁里面存了一个线程id，它总是让这个线程来执行被"锁住的"代码。这样就减少了竞争。以但是这又不适用于锁时间长且有大量线程竞争的情况。所以开发者才想到让锁`一步步膨胀(升级)`来逐步的适应并发情况。  
>++锁膨胀的顺序:无锁->偏向锁->轻量级锁(CAS)->重量级锁++

小结：
- 偏向锁：适用于无实际竞争，且将来将只有第一个申请锁的线程会使用锁
- 轻量级锁：适用于无实际竞争，多个线程交替使用锁，允许短时间的锁竞争
- 重量级锁：适用于有实际竞争，且锁竞争时间长

还要参考这里：https://www.jianshu.com/p/36eedeb3f912

### 由此引申出CAS
众所周知CAS是比较并交换，而且CAS又被称为是轻量级锁。那么它相当于synchronized这个重量级锁又轻在哪里了呢？

其实就是因为它的原理是比较并交换，伪代码如下：
```java
while(true) {
    Integer oldValue = getValue();
    Integer newValue = oldValue ++;
    if (compareAndSet(oldValue, newValue)) {
        break;
    }
}
```
其实就是三步：拿到旧值，计算新值，设置新值。但是在设置的时候会去比较旧值与传入的旧值相比是否已经发生了变动，如果没有就设置新值；否则重新循环判断。在JDK的Unsafe类里有实现，不过是底层的`C++`实现。(可以打开`C++`编译器然后搜索Unsafe.cpp即可)这也是这个CAS被称为自旋锁的原因。但是在单核的处理器上不存在真正的并行，所以线程如果不阻塞自己的话，旧owner就不能执行，锁也就永远得不到释放，此时不管自旋多久都是浪费。

这里扯到JVM顺便记一句：JVM确实是Java跨平台实现的原因，所以JVM的代码里即使是同一个方法但是如果使用了不同的CPU、操作系统，那么都会运行不同的代码，这就是跨平台了。(当然最后的执行结果肯定是一致的)

关于自旋，还有一种自适应自旋锁，所谓的自适应其实是为了更好了优化锁的**竞争时间**，它会判断让经常执行的线程更加容易获取到锁也就是说合理延长其自旋时间；而对于执行可能性比较小的线程则缩短其自旋时间或取消其自旋。且自旋时间会根据上一次的自旋时间来调整，这就更加符合我们希望“第二个线程在第一个线程执行完才去竞争锁”的思路。但是这个实现起来有一个难点就是很难确定合理的自旋时间。
#### CAS的问题
由于CAS的轻量，也同样为它带来了问题，那就是：
- 原子性问题
- ABA问题

##### 原子性问题
就是CAS其实是三步操作的整合，它这三步并不是原子性的。其实这样想是错误的，它的底层实现是调用了Unsafe类的compareAndSwap方法，这个方法是`C++`写的，简单看下就知道，它的内部会拿到当前运行机器的信息，然后返回一个`lock`指令，这个lock指令将会保证这三步的原子性，简而言之它`加了一把锁`，这个锁可能是缓存行锁又可能是总线锁。至于这两个锁想了解的话可以去搜一下。

##### ABA问题
由于CAS是通过旧值是否变动来判断是否应该设置新值的，所以如果有两次操作先把旧值修改掉，然后再把修改掉的值变回旧值，这样其他线程在判断的时候将会认为旧值没有变化，那么依然会设置新值进去，然而此时这个就已经稍稍偏离预期了。

解决方式就是加一个版本号，这样不论你操作了多少次，是否每次都操作回旧值，只要是版本号变了，那么就不去设置新值。